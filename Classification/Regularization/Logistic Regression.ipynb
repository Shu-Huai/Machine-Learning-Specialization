{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 逻辑回归\n",
    "\n",
    "在本练习中，您将实现逻辑回归并将其应用于两个不同的数据集。\n",
    "\n",
    "# 大纲\n",
    "\n",
    "- [1 包]()\n",
    "- [2 逻辑回归]()\n",
    "   - [2.1 问题陈述]()\n",
    "   - [2.2 加载和可视化数据]()\n",
    "   - [2.3 S型函数]()\n",
    "   - [2.4 逻辑回归的成本函数]()\n",
    "   - [2.5 逻辑回归的梯度]()\n",
    "   - [2.6 使用梯度下降学习参数 ]()\n",
    "   - [2.7 绘制决策边界]()\n",
    "   - [2.8 评估逻辑回归]()\n",
    "- [3 正则化逻辑回归]()\n",
    "   - [3.1 问题陈述]()\n",
    "   - [3.2 加载和可视化数据]()\n",
    "   - [3.3 特征映射]()\n",
    "   - [3.4 正则化逻辑回归的成本函数]()\n",
    "   - [3.5 正则化逻辑回归的梯度]()\n",
    "   - [3.6 使用梯度下降学习参数]()\n",
    "   - [3.7 绘制决策边界]()\n",
    "   - [3.8 评估正则化逻辑回归模型]()"
   ],
   "id": "af9c46113f9fdee3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 1 套餐\n",
    "\n",
    "首先，让我们运行下面的单元来导入您在此作业期间需要的所有包。\n",
    "\n",
    "- [NumPy](https://numpy.org)是使用Python进行科学计算的基础包。\n",
    "- [matplotlib](https://matplotlib.org)是一个著名的Python绘图库。\n",
    "- ``utils.py``包含此任务的辅助函数。您不需要修改此文件中的代码。"
   ],
   "id": "132b97ea8fe09f00"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:55:32.252615Z",
     "start_time": "2024-05-15T13:55:32.238110Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from utils import *\n",
    "\n",
    "%matplotlib inline"
   ],
   "id": "43574cadd030161c",
   "execution_count": 74,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 2 逻辑回归\n",
    "\n",
    "在练习的这一部分中，您将构建逻辑回归模型来预测学生是否被大学录取。\n",
    "\n",
    "### 2.1 问题陈述\n",
    "\n",
    "假设您是大学系的管理员，您想根据每个申请人的两次考试成绩来确定他们的录取机会。\n",
    "\n",
    "* 您拥有以前申请人的历史数据，可以将其用作逻辑回归的训练集。\n",
    "* 对于每个培训示例，您都有申请人两次考试的成绩和录取决定。\n",
    "* 您的任务是建立一个分类模型，根据这两次考试的分数来估计申请人的录取概率。\n",
    "\n",
    "### 2.2 加载和可视化数据\n",
    "\n",
    "您将首先加载此任务的数据集。\n",
    "\n",
    "- 下面显示的`load_dataset()`函数将数据加载到变量`X_train`和`y_train`中\n",
    "   - `X_train`包含学生两次考试的考试成绩\n",
    "   - `y_train`是录取决定\n",
    "       - 如果学生被录取，则`y_train = 1`\n",
    "       - 如果学生没有被录取，则`y_train = 0`\n",
    "   - `X_train`和`y_train`都是NumPy数组。"
   ],
   "id": "e7ce4681b35c15a7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.796818Z",
     "start_time": "2024-05-15T13:54:08.788121Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train = load_data(\"data/ex2data1.txt\")",
   "id": "e7998c60683056dc",
   "execution_count": 42,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 查看变量\n",
    "\n",
    "让我们更熟悉您的数据集。\n",
    "\n",
    "- 一个好的起点是打印出每个变量并查看它包含的内容。\n",
    "\n",
    "下面的代码打印`X_train`的前五个值和变量的类型。"
   ],
   "id": "83e1c12f3790a7d6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.811826Z",
     "start_time": "2024-05-15T13:54:08.797821Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"First five elements in X_train are:\\n\", X_train[:5])\n",
    "print(\"Type of X_train:\", type(X_train))"
   ],
   "id": "c67c25bb9e32b624",
   "execution_count": 43,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在打印`y_train`的前五个值",
   "id": "e5accf055a5fde38"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.827334Z",
     "start_time": "2024-05-15T13:54:08.812827Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"First five elements in y_train are:\\n\", y_train[:5])\n",
    "print(\"Type of y_train:\", type(y_train))"
   ],
   "id": "d2fced67c66c6f31",
   "execution_count": 44,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 检查变量的维度\n",
    "\n",
    "熟悉数据的另一个有用方法是查看其维度。让我们打印`X_train`和`y_train`的形状，看看数据集中有多少训练样本。"
   ],
   "id": "6454a80e43ed156d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.842844Z",
     "start_time": "2024-05-15T13:54:08.828333Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('The shape of X_train is: ' + str(X_train.shape))\n",
    "print('The shape of y_train is: ' + str(y_train.shape))\n",
    "print('We have m = %d training examples' % (len(y_train)))"
   ],
   "id": "b688750735dc7467",
   "execution_count": 45,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 可视化您的数据\n",
    "\n",
    "在开始实现任何学习算法之前，如果可能的话，将数据可视化总是好的。\n",
    "- 下面的代码在二维图上显示数据（如下所示），其中轴是两次考试成绩，正例和负例用不同的标记显示。\n",
    "- 我们使用 utils.py 文件中的辅助函数来生成此图。\n",
    "\n",
    "<img src=\"../../images/figure 1.png\" width=\"450\" height=\"450\">"
   ],
   "id": "6ca0e1997e4785b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.919754Z",
     "start_time": "2024-05-15T13:54:08.843845Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_data(X_train, y_train[:], pos_label=\"Admitted\", neg_label=\"Not admitted\")\n",
    "plt.ylabel('Exam 2 score')\n",
    "plt.xlabel('Exam 1 score')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ],
   "id": "64010e90ba36b5ab",
   "execution_count": 46,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "您的目标是构建一个逻辑回归模型来拟合这些数据。\n",
    "\n",
    "- 通过此模型，您可以根据新生的两次考试成绩来预测新生是否会被录取。"
   ],
   "id": "c7b3a47871a2082c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.3 S形函数\n",
    "\n",
    "回想一下，对于逻辑回归，模型表示为\n",
    "\n",
    "$$f_{\\mathbf{w},b}(x)=g(\\mathbf{w}\\cdot\\mathbf{x}+b)$$\n",
    "\n",
    "其中函数$g$是sigmoid函数。sigmoid函数定义为：\n",
    "\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "让我们首先实现sigmoid函数，以便本作业的其余部分可以使用它。\n",
    "\n",
    "### 练习1\n",
    "请完成`sigmoid`函数进行计算\n",
    "\n",
    "$$g(z)=\\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "注意\n",
    "\n",
    "- `z`并不总是单个数字，也可以是数字数组。\n",
    "- 如果输入是数字数组，我们希望将sigmoid函数应用于输入数组中的每个值。\n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后面提供的提示来帮助您实施。"
   ],
   "id": "917c740801b63b08"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.935265Z",
     "start_time": "2024-05-15T13:54:08.920753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def sigmoid(z):\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    return g"
   ],
   "id": "798c112a3cfddc18",
   "execution_count": 47,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "完成后，尝试通过调用下面单元格中的`sigmoid(x)`来测试一些值。\n",
    "\n",
    "- 对于x较大的正值，sigmoid应接近1，而对于较大的负值，sigmoid应接近0。\n",
    "- 评估`sigmoid(0)`应该正好为 0.5。"
   ],
   "id": "563affa88221116e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.950764Z",
     "start_time": "2024-05-15T13:54:08.936265Z"
    }
   },
   "cell_type": "code",
   "source": "print(\"sigmoid(0) = \" + str(sigmoid(0)))",
   "id": "d8cde46f0fd0012c",
   "execution_count": 48,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                sigmoid(0)\n",
    "            <b>\n",
    "        </td>\n",
    "        <td>\n",
    "            0.5\n",
    "        </td> \n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "- 如前所述，您的代码还应该适用于向量和矩阵。对于矩阵，您的函数应该对每个元素执行sigmoid函数。"
   ],
   "id": "55eb68e6c0fa4931"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:58:52.689133Z",
     "start_time": "2024-05-15T13:58:52.684133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"sigmoid([ -1, 0, 1, 2]) = \" + str(sigmoid(np.array([-1, 0, 1, 2]))))\n",
    "from public_tests import *\n",
    "\n",
    "sigmoid_test(sigmoid)"
   ],
   "id": "7d0e00d2c23f182d",
   "execution_count": 75,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                sigmoid([-1, 0, 1, 2])\n",
    "            <b>\n",
    "        </td> \n",
    "        <td>\n",
    "            [0.26894142        0.5           0.73105858        0.88079708]\n",
    "        </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "df13c6c491ab97c3"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.4 逻辑回归的成本函数\n",
    "\n",
    "在本部分中，您将实现逻辑回归的成本函数。\n",
    "\n",
    "### 练习2\n",
    "\n",
    "请使用下面的等式完成`compute_cost`函数。\n",
    "\n",
    "回想一下，对于逻辑回归，成本函数的形式为\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),y^{(i)})\\right]\\tag{1}$$\n",
    "\n",
    "在哪里\n",
    "\n",
    "* m是数据集中训练样本的数量\n",
    "* $loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),y^{(i)})$是单个数据点的成本，即——\n",
    "  $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}),y^{(i)})=-y^{(i)}\\log\\left(f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right)\\log\\left(1-f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)\\tag{2}$$\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$是模型的预测，而$y^{(i)}$是实际标签\n",
    "* $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})=g(\\mathbf{w}\\cdot\\mathbf{x^{(i)}}+b)$其中函数$g$是sigmoid函数。\n",
    "    * 首先计算一个中间变量$z_{\\mathbf{w},b}(\\mathbf{x}^{(i)})=\\mathbf{w}\\cdot\\mathbf{x^{(i)}}+b=w_0x^{(i)}_0+...+w_{n-1}x^{(i)}_{n-1}+b$其中$n$是特征，在计算$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})=g(z_{\\mathbf{w},b}(\\mathbf{x}^{(i)}))$\n",
    "\n",
    "笔记：\n",
    "\n",
    "* 当您执行此操作时，请记住变量`X_train`和`y_train`不是标量值，而是形状分别为`(m, n)`和`(m, 1)`的矩阵，其中$n$是数字特征数，$m$是训练样本的数量。\n",
    "* 您可以使用上面为这部分实现的sigmoid函数。\n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后面提供的提示来帮助您实施。"
   ],
   "id": "6a1ed89b18f196a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.981415Z",
     "start_time": "2024-05-15T13:54:08.966411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost(X, y, w, b, _=1):\n",
    "    m, n = X.shape\n",
    "    total_cost = 0\n",
    "    for i in range(m):\n",
    "        f = sigmoid(X[i].dot(w) + b)\n",
    "        total_cost += -y[i] * np.log(f) - (1 - y[i]) * np.log(1 - f)\n",
    "    return total_cost / m"
   ],
   "id": "2ddf42a4917662aa",
   "execution_count": 50,
   "outputs": []
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:08.996924Z",
     "start_time": "2024-05-15T13:54:08.982415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "m, n = X_train.shape\n",
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "cost = compute_cost(X_train, y_train, initial_w, initial_b)\n",
    "print('Cost at initial w (zeros): {:.3f}'.format(cost))"
   ],
   "id": "b7e3551c6d6bc03c",
   "execution_count": 51,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Cost at initial w (zeros)\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            0.693\n",
    "        </td> \n",
    "  </tr>\n",
    "</table>"
   ],
   "id": "73d0e19b2bdc4392"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:09.012495Z",
     "start_time": "2024-05-15T13:54:08.998423Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_w = np.array([0.2, 0.2])\n",
    "test_b = -24.\n",
    "cost = compute_cost(X_train, y_train, test_w, test_b)\n",
    "print('Cost at test w,b: {:.3f}'.format(cost))\n",
    "compute_cost_test(compute_cost)"
   ],
   "id": "653d231db129eb56",
   "execution_count": 52,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Cost at test w,b\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            0.218\n",
    "        </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "ec209ed18946bd1f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.5 逻辑回归的梯度\n",
    "\n",
    "在本节中，您将实现逻辑回归的梯度。\n",
    "\n",
    "回想一下，梯度下降算法是：\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:}\\;\\lbrace\\\\\n",
    "& \\;\\;\\;w_j=w_j-\\alpha\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}\\;&\\text{for j := 0..n-1}\\tag{1}\\\\ \n",
    "& \\;\\;\\;\\;\\;b=b-\\alpha\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中每次迭代对所有$j$的$w_j$执行同时更新。"
   ],
   "id": "c9b484666a887ac2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 练习 3\n",
    "\n",
    "请完成`compute_gradient`函数来计算$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$,$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$来自下面的方程（2）和（3）。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})x_{j}^{(i)}\\tag{3}\n",
    "$$\n",
    "\n",
    "* m是数据集中训练样本的数量\n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$ 是模型的预测，而 $y^{(i)}$ 是实际标签\n",
    "- **注意**：虽然这个梯度看起来与线性回归梯度相同，但公式实际上是不同的，因为线性和逻辑回归对$f_{\\mathbf{w},b}(x)$的定义不同。\n",
    "\n",
    "和以前一样，您可以使用上面实现的sigmoid函数，如果遇到困难，您可以查看下面单元格后面出现的提示来帮助您实现。"
   ],
   "id": "fcdbf92d241532e7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:09.027995Z",
     "start_time": "2024-05-15T13:54:09.013498Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient(X, y, w, b, _=None):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "        f = sigmoid(X[i].dot(w) + b)\n",
    "        dj_dw += (f - y[i]) * X[i]\n",
    "        dj_db += (f - y[i])\n",
    "    return dj_db / m, dj_dw / m"
   ],
   "id": "48f3ff39727a8c45",
   "execution_count": 53,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格来检查`compute_gradient`函数的实现，其中参数$w$有两种不同的初始化",
   "id": "bd43d2bcc93614fb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:09.043635Z",
     "start_time": "2024-05-15T13:54:09.029495Z"
    }
   },
   "cell_type": "code",
   "source": [
    "initial_w = np.zeros(n)\n",
    "initial_b = 0.\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, initial_w, initial_b)\n",
    "print(f'dj_db at initial w (zeros):{dj_db}')\n",
    "print(f'dj_dw at initial w (zeros):{dj_dw.tolist()}')"
   ],
   "id": "97dde7130b391ed1",
   "execution_count": 54,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                dj_db at initial w (zeros)\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            -0.1\n",
    "         </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                dj_dw at initial w (zeros):\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            [-12.00921658929115, -11.262842205513591]\n",
    "         </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "6b23cbb98a3da3b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:09.058644Z",
     "start_time": "2024-05-15T13:54:09.044638Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test_w = np.array([0.2, -0.5])\n",
    "test_b = -24\n",
    "dj_db, dj_dw = compute_gradient(X_train, y_train, test_w, test_b)\n",
    "print('dj_db at test_w:', dj_db)\n",
    "print('dj_dw at test_w:', dj_dw.tolist())\n",
    "compute_gradient_test(compute_gradient)"
   ],
   "id": "1133fbd9c683582f",
   "execution_count": 55,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "<table>\n",
    "    <tr>\n",
    "        <td> \n",
    "            <b>\n",
    "                dj_db at initial w (zeros)\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            -0.5999999999991071 \n",
    "        </td> \n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                ddj_dw at initial w (zeros):\n",
    "            </b>\n",
    "        </td>\n",
    "    <td>\n",
    "        [-44.8313536178737957, -44.37384124953978] \n",
    "    </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "35f2de96781a4e2a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.6 使用梯度下降学习参数\n",
    "\n",
    "与之前的作业类似，您现在将使用梯度下降找到逻辑回归模型的最佳参数。\n",
    "\n",
    "- 您不需要为此部分实现任何内容。 只需运行下面的单元格即可。\n",
    "- 验证梯度下降是否正常工作的一个好方法是查看\n",
    "  \n",
    "  $J(\\mathbf{w},b)$的值并检查它是否随着每一步而减少。\n",
    "- 假设您已经实现了梯度并正确计算了成本，您的$J(\\mathbf{w},b)$值不应增加，并且应在算法结束时收敛到稳定值。"
   ],
   "id": "d5690262411bfe09"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:09.073874Z",
     "start_time": "2024-05-15T13:54:09.059644Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_):\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)\n",
    "        w_in = w_in - alpha * dj_dw\n",
    "        b_in = b_in - alpha * dj_db\n",
    "        if i < 100000:\n",
    "            cost = cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "        if i % math.ceil(num_iters / 10) == 0 or i == (num_iters - 1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "    return w_in, b_in, J_history, w_history"
   ],
   "id": "ffdcdca611d0df07",
   "execution_count": 56,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "现在让我们运行上面的梯度下降算法来学习数据集的参数。\n",
    "\n",
    "**笔记**\n",
    "\n",
    "下面的代码块需要几分钟才能运行，尤其是对于非矢量化版本。您可以减少`iterations`来测试您的实现并加快迭代速度。如果您有时间，请尝试运行100,000次迭代以获得更好的结果。"
   ],
   "id": "9a60513f691bede6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.464033Z",
     "start_time": "2024-05-15T13:54:09.075374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "intial_w = 0.01 * (np.random.rand(2).reshape(-1, 1) - 0.5)\n",
    "initial_b = -8\n",
    "iterations = 100000\n",
    "alpha = 0.001\n",
    "w, b, J_history, _ = gradient_descent(X_train, y_train, initial_w, initial_b, compute_cost, compute_gradient, alpha,\n",
    "                                      iterations, 0)"
   ],
   "id": "db36f51cc579bd3",
   "execution_count": 57,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.7 绘制决策边界\n",
    "\n",
    "我们现在将使用梯度下降的最终参数来绘制线性拟合。如果您正确实现了前面的部分，您应该看到以下图：\n",
    "\n",
    "<img src=\"../../images/figure 2.png\" width=\"450\" height=\"450\">\n",
    "\n",
    "我们将使用`utils.py`文件中的辅助函数来创建此图。"
   ],
   "id": "f70851915b20383d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.541091Z",
     "start_time": "2024-05-15T13:54:46.465035Z"
    }
   },
   "cell_type": "code",
   "source": "plot_decision_boundary(w, b, X_train, y_train)",
   "id": "cabca9d99a5b7e21",
   "execution_count": 58,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2.8 评估逻辑回归\n",
    "\n",
    "我们可以通过查看学习模型对训练集的预测效果来评估我们找到的参数的质量。\n",
    "\n",
    "您将实现下面的`predict`函数来执行此操作。"
   ],
   "id": "f226a94afd58faa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 练习 4\n",
    "\n",
    "请完成`predict`函数，以在给定数据集和学习参数向量$w$和$b$的情况下生成`1`或`0`预测。\n",
    "\n",
    "- 首先，您需要根据模型$f(x^{(i)})=g(w\\cdot x^{(i)})$计算每个示例的预测\n",
    "    - 你之前已经在上面的部分中实现过这个\n",
    "- 我们将模型的输出（$f(x^{(i)})$）解释为给定$x^{(i)}$且参数化为$y^{(i)}=1$的概率$w$。\n",
    "- 因此，要从逻辑回归模型获得最终预测（$y^{(i)}=0$或$y^{(i)}=1$），您可以使用以下启发式——\n",
    "\n",
    "   如果$f(x^{(i)})>=0.5$，则预测$y^{(i)}=1$\n",
    "\n",
    "   如果$f(x^{(i)})<0.5$，则预测$y^{(i)}=0$\n",
    "    \n",
    "如果您遇到困难，可以查看下面单元格后面提供的提示来帮助您实施。"
   ],
   "id": "e8000b47d0131cf2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.556261Z",
     "start_time": "2024-05-15T13:54:46.542253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def predict(X, w, b):\n",
    "    m, n = X.shape\n",
    "    p = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        f = sigmoid(X[i].dot(w) + b)\n",
    "        p[i] = 1 if f >= 0.5 else 0\n",
    "    return p"
   ],
   "id": "85b9acc23d53b026",
   "execution_count": 59,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "完成`predict`函数后，让我们运行下面的代码，通过计算正确示例的百分比来报告分类器的训练准确性。",
   "id": "3aa560ba4e2312ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.571766Z",
     "start_time": "2024-05-15T13:54:46.557263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "tmp_w = np.random.randn(2)\n",
    "tmp_b = 0.3\n",
    "tmp_X = np.random.randn(4, 2) - 0.5\n",
    "tmp_p = predict(tmp_X, tmp_w, tmp_b)\n",
    "print(f'Output of predict: shape {tmp_p.shape}, value {tmp_p}')\n",
    "predict_test(predict)"
   ],
   "id": "2100c083c80d5e61",
   "execution_count": 60,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出** \n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Output of predict: shape (4,),value [0. 1. 1. 1.]\n",
    "            </b>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "8d2dceb091cc2237"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "现在让我们用它来计算训练集的准确性",
   "id": "45b9750fbd882ffe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.587272Z",
     "start_time": "2024-05-15T13:54:46.573267Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = predict(X_train, w, b)\n",
    "print('Train Accuracy: %f' % (np.mean(p == y_train) * 100))"
   ],
   "id": "8e0a2ce16bd6a476",
   "execution_count": 61,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Train Accuracy (approx):\n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            92.00\n",
    "         </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "b3162820db9a2faf"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3 正则化逻辑回归\n",
    "\n",
    "在练习的这一部分中，您将实施正则化逻辑回归来预测制造工厂的微芯片是否通过质量保证（QA）。在质量检查期间，每个微芯片都会经过各种测试，以确保其正常运行。\n",
    "\n",
    "### 3.1 问题陈述\n",
    "\n",
    "假设您是工厂的产品经理，您有一些微芯片在两次不同测试中的测试结果。\n",
    "\n",
    "- 通过这两项测试，您想确定是否应该接受或拒绝微芯片。\n",
    "- 为了帮助您做出决定，您拥有过去微芯片的测试结果数据集，您可以从中构建逻辑回归模型。\n",
    "\n",
    "### 3.2 加载和可视化数据\n",
    "\n",
    "与本练习的前面部分类似，我们首先加载此任务的数据集并对其进行可视化。\n",
    "\n",
    "- 下面显示的`load_dataset()`函数将数据加载到变量`X_train`和`y_train`中\n",
    "   - `X_train`包含两次测试中微芯片的测试结果\n",
    "   - `y_train`包含QA的结果\n",
    "       - 如果微芯片被接受，则`y_train = 1`\n",
    "       - 如果微芯片被拒绝，则`y_train = 0`\n",
    "   - `X_train`和`y_train`都是NumPy数组。"
   ],
   "id": "f72bf564af96b619"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.602773Z",
     "start_time": "2024-05-15T13:54:46.588273Z"
    }
   },
   "cell_type": "code",
   "source": "X_train, y_train = load_data(\"data/ex2data2.txt\")",
   "id": "37b607cc6a08a14",
   "execution_count": 62,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 查看变量\n",
    "\n",
    "下面的代码打印`X_train`和`y_train`的前五个值以及变量的类型。"
   ],
   "id": "29d52e73828db9a1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.618390Z",
     "start_time": "2024-05-15T13:54:46.603381Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X_train:\", X_train[:5])\n",
    "print(\"Type of X_train:\", type(X_train))\n",
    "print(\"y_train:\", y_train[:5])\n",
    "print(\"Type of y_train:\", type(y_train))"
   ],
   "id": "44c00285250f27d6",
   "execution_count": 63,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 检查变量的维度\n",
    "\n",
    "熟悉数据的另一个有用方法是查看其维度。让我们打印`X_train`和`y_train`的形状，看看数据集中有多少训练样本。"
   ],
   "id": "d4aef8c7af291d5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.633994Z",
     "start_time": "2024-05-15T13:54:46.619389Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print('The shape of X_train is: ' + str(X_train.shape))\n",
    "print('The shape of y_train is: ' + str(y_train.shape))\n",
    "print('We have m = %d training examples' % (len(y_train)))"
   ],
   "id": "ecc2c5836cf640c1",
   "execution_count": 64,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 可视化您的数据\n",
    "\n",
    "辅助函数`plot_data`（来自`utils.py`）用于生成如图3所示的图形，其中轴是两个测试分数，以及正数（y=1，接受）和负数（y=0，被拒绝）示例用不同的标记显示。\n",
    "\n",
    "<img src=\"../../images/figure 3.png\" width=\"450\" height=\"450\">"
   ],
   "id": "1c99eb1389d1b1e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.696018Z",
     "start_time": "2024-05-15T13:54:46.634997Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plot_data(X_train, y_train[:], pos_label=\"Accepted\", neg_label=\"Rejected\")\n",
    "plt.ylabel('Microchip Test 2')\n",
    "plt.xlabel('Microchip Test 1')\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.show()"
   ],
   "id": "ba4d5c49cf7a373",
   "execution_count": 65,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "图3显示我们的数据集无法通过图中的直线分为正例和负例。因此，逻辑回归的直接应用在此数据集上不会表现良好，因为逻辑回归只能找到线性决策边界。",
   "id": "596bc0d830dcac3d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.3 特征映射\n",
    "\n",
    "更好地拟合数据的一种方法是从每个数据点创建更多特征。在提供的函数`map_feature`中，我们将把特征映射到$x_1$和$x_2$的所有多项式项中，直至六次方。\n",
    "\n",
    "$$\\mathrm{map\\_feature}(x)= \n",
    "\\left[\\begin{array}{c}\n",
    "x_1\\\\\n",
    "x_2\\\\\n",
    "x_1^2\\\\\n",
    "x_1 x_2\\\\\n",
    "x_2^2\\\\\n",
    "x_1^3\\\\\n",
    "\\vdots\\\\\n",
    "x_1x_2^5\\\\\n",
    "x_2^6\\end{array}\\right]$$\n",
    "\n",
    "作为此映射的结果，我们的两个特征向量（两次QA测试的分数）已转换为27维向量。\n",
    "\n",
    "- 在这个高维特征向量上训练的逻辑回归分类器将具有更复杂的决策边界，并且在我们的二维图中绘制时将是非线性的。\n",
    "- 我们在utils.py中为您提供了`map_feature`函数。"
   ],
   "id": "f15c041ff16c8682"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.711642Z",
     "start_time": "2024-05-15T13:54:46.696639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Original shape of data:\", X_train.shape)\n",
    "mapped_X = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "print(\"Shape after feature mapping:\", mapped_X.shape)"
   ],
   "id": "a4241c5e978bdfc4",
   "execution_count": 66,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "我们还打印`X_train`和`mapped_X`的第一个元素来查看转换。",
   "id": "f82b5de583ea6e3a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.727155Z",
     "start_time": "2024-05-15T13:54:46.712642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"X_train[0]:\", X_train[0])\n",
    "print(\"mapped X_train[0]:\", mapped_X[0])"
   ],
   "id": "6944e3385195e78d",
   "execution_count": 67,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "虽然特征映射允许我们构建更具表现力的分类器，但它也更容易出现过度拟合。在练习的下一部分中，您将实施正则化逻辑回归来拟合数据，并亲自了解正则化如何帮助解决过度拟合问题。\n",
    "\n",
    "### 3.4 正则化逻辑回归的成本函数\n",
    "\n",
    "在这一部分中，您将实现正则化逻辑回归的成本函数。\n",
    "\n",
    "回想一下，对于正则化逻辑回归，成本函数的形式为\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[-y^{(i)}\\log\\left(f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right)\\log\\left(1-f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2$$\n",
    "\n",
    "将此与没有正则化的成本函数（您在上面实现的）进行比较，其形式为\n",
    "\n",
    "$$J(\\mathbf{w}.b)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[-y^{(i)}\\log\\left(f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right)\\log\\left(1-f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)\\right]$$\n",
    "\n",
    "区别在于正则化项，即$$\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2$$\n",
    "\n",
    "请注意，$b$参数未正则化。"
   ],
   "id": "92f85b87f8d14fb4"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 练习 5\n",
    "\n",
    "请完成下面的`compute_cost_reg`函数来计算$w$中每个元素的以下项\n",
    "\n",
    "$$\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2$$\n",
    "\n",
    "然后，起始代码将其添加到没有正则化的成本中（您在上面的`compute_cost`中计算）以计算调整后的成本。\n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后面提供的提示来帮助您实施。"
   ],
   "id": "5affd05e0b58eda5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.742599Z",
     "start_time": "2024-05-15T13:54:46.728006Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost_reg(X, y, w, b, lambda_=1):\n",
    "    m, n = X.shape\n",
    "    cost_without_reg = compute_cost(X, y, w, b)\n",
    "    reg_cost = np.sum(np.power(w, 2))\n",
    "    total_cost = cost_without_reg + (lambda_ / (2 * m)) * reg_cost\n",
    "    return total_cost"
   ],
   "id": "3b5c8becc6f44d35",
   "execution_count": 68,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格来检查`compute_cost_reg`函数的实现。",
   "id": "cd6c9a934016daa4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.758016Z",
     "start_time": "2024-05-15T13:54:46.743010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "cost = compute_cost_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "print(\"Regularized cost :\", cost)\n",
    "compute_cost_reg_test(compute_cost_reg)"
   ],
   "id": "df466a2d1ab03c",
   "execution_count": 69,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Regularized cost : \n",
    "            </b>\n",
    "        </td>\n",
    "        <td>\n",
    "            0.6618252552483948 \n",
    "        </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "d8a58295dccfa846"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.5 正则化逻辑回归的梯度\n",
    "\n",
    "在本节中，您将实现正则化逻辑回归的梯度。\n",
    "\n",
    "正则化成本函数的梯度有两个组成部分。第一个$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$是标量，另一个是与参数$\\mathbf{w}$形状相同的向量，其中$j^\\mathrm{th}$元素定义如下：\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}=\\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})$$\n",
    "\n",
    "$$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} =\\left(\\frac{1}{m}\\sum_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})x_j^{(i)}\\right)+\\frac{\\lambda}{m}w_j\\quad\\,\\mbox{for $j=0\\cdots(n-1)$}$$\n",
    "\n",
    "将此与没有正则化的成本函数的梯度（您在上面实现的）进行比较，其形式为\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})\\tag{2}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-\\mathbf{y}^{(i)})x_{j}^{(i)}\\tag{3}\n",
    "$$\n",
    "\n",
    "可以看到，$\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}$是一样的，区别在于$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$，即$$\\frac{\\lambda}{m}w_j\\quad\\,\\mbox{for $j=0\\cdots(n-1)$}$$"
   ],
   "id": "f7b78482bc2ca5dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 练习 6\n",
    "\n",
    "请完成下面的`compute_gradient_reg`函数，修改下面的代码来计算以下项\n",
    "\n",
    "$$\\frac{\\lambda}{m}w_j\\quad\\,\\mbox{for $j=0\\cdots(n-1)$}$$\n",
    "\n",
    "起始代码会将此项添加到从上面的`compute_gradient`返回的$\\frac{\\partial J(\\mathbf{w},b)}{\\partial w}$中，以获得正则化成本函数的梯度。\n",
    "\n",
    "如果您遇到困难，可以查看下面单元格后面提供的提示来帮助您实施。"
   ],
   "id": "8c92485b34907f15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.773533Z",
     "start_time": "2024-05-15T13:54:46.759017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_=1):\n",
    "    m, n = X.shape\n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "    dj_dw += lambda_ / m * w\n",
    "    return dj_db, dj_dw"
   ],
   "id": "29ee508c01aa7dca",
   "execution_count": 70,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格来检查`compute_gradient_reg`函数的实现。",
   "id": "eba39d85477d4bf5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:54:46.789033Z",
     "start_time": "2024-05-15T13:54:46.774533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_mapped = map_feature(X_train[:, 0], X_train[:, 1])\n",
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 0.5\n",
    "lambda_ = 0.5\n",
    "dj_db, dj_dw = compute_gradient_reg(X_mapped, y_train, initial_w, initial_b, lambda_)\n",
    "print(f\"dj_db: {dj_db}\", )\n",
    "print(f\"First few elements of regularized dj_dw:\\n {dj_dw[:4].tolist()}\", )\n",
    "compute_gradient_reg_test(compute_gradient_reg)"
   ],
   "id": "cbf107656df7f1ef",
   "execution_count": 71,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                dj_db:\n",
    "            </b>\n",
    "            0.07138288792343656\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>     \n",
    "            <b>\n",
    "                First few elements of regularized dj_dw:\n",
    "            </b>\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td> \n",
    "            [[-0.010386028450548701], [0.01140985288328012], [0.0536273463274574], [0.003140278267313462]] \n",
    "        </td> \n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "9c33a8d61da94fec"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.6 使用梯度下降学习参数\n",
    "\n",
    "与前面的部分类似，您将使用上面实现的梯度下降函数来学习最佳参数$w$,$b$。\n",
    "\n",
    "- 如果您已正确完成正则化逻辑回归的成本和梯度，您应该能够单步执行下一个单元格以学习参数$w$。\n",
    "- 训练参数后，我们将用它来绘制决策边界。\n",
    "\n",
    "**笔记**\n",
    "\n",
    "下面的代码块需要相当长的时间才能运行，特别是对于非矢量化版本。您可以减少`iterations`来测试您的实现并加快迭代速度。如果您有时间，请运行100,000次迭代以获得更好的结果。"
   ],
   "id": "e07bad11d91de666"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:55:32.221856Z",
     "start_time": "2024-05-15T13:54:46.790533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "initial_w = np.random.rand(X_mapped.shape[1]) - 0.5\n",
    "initial_b = 1.\n",
    "lambda_ = 0.01\n",
    "iterations = 100000\n",
    "alpha = 0.01\n",
    "w, b, J_history, _ = gradient_descent(X_mapped, y_train, initial_w, initial_b, compute_cost_reg, compute_gradient_reg,\n",
    "                                      alpha, iterations, lambda_)"
   ],
   "id": "53231e65ee0815f9",
   "execution_count": 72,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.7 绘制决策边界\n",
    "\n",
    "为了帮助您可视化该分类器学习的模型，我们将使用`plot_decision_boundary`函数来绘制分隔正例和负例的（非线性）决策边界。\n",
    "\n",
    "- 在函数中，我们通过在均匀间隔的网格上计算分类器的预测来绘制非线性决策边界，然后绘制预测从y=0到y=1变化的等高线图。\n",
    "- 学习参数$w$、$b$后，下一步是绘制类似于图4的决策边界。\n",
    "\n",
    "<img src=\"../../images/figure 4.png\" width=\"450\" height=\"450\">"
   ],
   "id": "d812b5ef98defc23"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3.8 评估正则化逻辑回归模型\n",
    "\n",
    "您将使用上面实现的`predict`函数来计算训练集上的规范化逻辑回归模型的准确性"
   ],
   "id": "25b9c8d391dca72"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-15T13:55:32.237355Z",
     "start_time": "2024-05-15T13:55:32.222856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "p = predict(X_mapped, w, b)\n",
    "print('Train Accuracy: %f' % (np.mean(p == y_train) * 100))"
   ],
   "id": "9c3111529bcecde1",
   "execution_count": 73,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Train Accuracy:\n",
    "            </b>\n",
    "            ~ 80%\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "2a76de06a04963ab"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
