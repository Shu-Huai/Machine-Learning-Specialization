{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 可选实验室 - 正则化成本和梯度",
   "id": "74dc781dfded25b7"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 目标\n",
    "\n",
    "在本实验室中，您将：\n",
    "\n",
    "- 使用正则化项扩展先前的线性和逻辑成本函数。\n",
    "- 重新运行前面的过度拟合示例，并添加正则化项。"
   ],
   "id": "6604597fc573286e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.464283Z",
     "start_time": "2024-05-14T15:18:38.445274Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from plt_overfit import overfit_example, output\n",
    "from lab_utils_common import sigmoid\n",
    "\n",
    "np.set_printoptions(precision=8)"
   ],
   "id": "42f1cef2b1cd5de",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# 添加正则化\n",
    "\n",
    "<img align=\"Left\" src=\"../../images/C1_W3_LinearGradientRegularized.png\" style=\" width:400px; padding: 10px; \" >\n",
    "\n",
    "<img align=\"Center\" src=\"../../images/C1_W3_LogisticGradientRegularized.png\" style=\" width:400px; padding: 10px; \" >\n",
    "\n",
    "上面的幻灯片显示了线性回归和逻辑回归的成本函数和梯度函数。笔记：\n",
    "\n",
    "- 成本\n",
    "    - 线性回归和逻辑回归之间的成本函数显着不同，但在方程中添加正则化是相同的。\n",
    "- 坡度\n",
    "    - 线性回归和逻辑回归的梯度函数非常相似。它们的区别仅在于$f_{wb}$的实现。"
   ],
   "id": "f3df4fe49049b328"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 正则化成本函数\n",
    "\n",
    "### 正则化线性回归的成本函数\n",
    "\n",
    "成本函数正则化线性回归的方程为：\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{2m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})^2+\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2\\tag{1}$$\n",
    "\n",
    "在哪里：\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})=\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b\\tag{2}$$\n",
    "\n",
    "将此与没有正则化的成本函数（您在之前的实验室中实现）进行比较，其形式为：\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{2m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})^2$$\n",
    "\n",
    "区别在于正则化项\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2$\n",
    "</span>\n",
    "    \n",
    "包含此项会激励梯度下降以最小化参数的大小。请注意，在此示例中，参数$b$未正则化。这是标准做法。\n",
    "\n",
    "下面是等式（1）和（2）的实现。请注意，这使用了*本课程的标准模式*，即所有`m`示例上的`for loop`。"
   ],
   "id": "c6d492efe088a7bf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.526318Z",
     "start_time": "2024-05-14T15:18:38.517319Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost_linear_reg(X, y, w, b, lambda_=1):\n",
    "    m = X.shape[0]\n",
    "    n = len(w)\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = np.dot(X[i], w) + b\n",
    "        cost = cost + (f_wb_i - y[i]) ** 2\n",
    "    cost = cost / (2 * m)\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j] ** 2)\n",
    "    reg_cost = (lambda_ / (2 * m)) * reg_cost\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ],
   "id": "de2d5fe1b2b5c6c8",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格以查看其实际情况。",
   "id": "f2ffdd672ff11020"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.541827Z",
     "start_time": "2024-05-14T15:18:38.528320Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5, 6)\n",
    "y_tmp = np.array([0, 1, 0, 1, 0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1, ) - 0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ],
   "id": "e8572ff0f31da93a",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**预期输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Regularized cost: \n",
    "            </b>\n",
    "            0.07917239320214275\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "59718f60bb961ce0"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 正则化逻辑回归的成本函数\n",
    "\n",
    "对于正则化**logistic**回归，成本函数的形式为\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[-y^{(i)}\\log\\left(f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right)\\log\\left(1-f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)\\right]+\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2\\tag{3}$$\n",
    "\n",
    "在哪里：\n",
    "\n",
    "$$f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})=sigmoid(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b)\\tag{4}$$\n",
    "\n",
    "将此与没有正则化的成本函数（您在之前的实验室中实现）进行比较：\n",
    "\n",
    "$$J(\\mathbf{w},b)=\\frac{1}{m}\\sum_{i=0}^{m-1}\\left[-y^{(i)}\\log\\left(f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)-\\left(1-y^{(i)}\\right)\\log\\left(1-f_{\\mathbf{w},b}\\left(\\mathbf{x}^{(i)}\\right)\\right)\\right]$$\n",
    "\n",
    "与上面线性回归的情况一样，区别在于正则化项，即\n",
    "\n",
    "<span style=\"color:blue\">\n",
    "    $\\frac{\\lambda}{2m}\\sum_{j=0}^{n-1}w_j^2$\n",
    "</span>\n",
    "\n",
    "包含此项会激励梯度下降以最小化参数的大小。请注意，在此示例中，参数$b$未正则化。这是标准做法。"
   ],
   "id": "fdf286a07da59421"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.557327Z",
     "start_time": "2024-05-14T15:18:38.542828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_cost_logistic_reg(X, y, w, b, lambda_=1):\n",
    "    m, n = X.shape\n",
    "    cost = 0.\n",
    "    for i in range(m):\n",
    "        z_i = np.dot(X[i], w) + b\n",
    "        f_wb_i = sigmoid(z_i)\n",
    "        cost += -y[i] * np.log(f_wb_i) - (1 - y[i]) * np.log(1 - f_wb_i)\n",
    "    cost = cost / m\n",
    "    reg_cost = 0\n",
    "    for j in range(n):\n",
    "        reg_cost += (w[j] ** 2)\n",
    "    reg_cost = (lambda_ / (2 * m)) * reg_cost\n",
    "    total_cost = cost + reg_cost\n",
    "    return total_cost"
   ],
   "id": "55ff2f7c02554dd3",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格以查看其实际情况。",
   "id": "625020c90bd6e843"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.572838Z",
     "start_time": "2024-05-14T15:18:38.558828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5, 6)\n",
    "y_tmp = np.array([0, 1, 0, 1, 0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1]).reshape(-1, ) - 0.5\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "cost_tmp = compute_cost_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(\"Regularized cost:\", cost_tmp)"
   ],
   "id": "3d3b7e81c5f42133",
   "execution_count": 15,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**预期输出**：\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <b>\n",
    "                Regularized cost:\n",
    "            </b>\n",
    "        0.6850849138741673\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ],
   "id": "b545079cb191d375"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 正则化梯度下降\n",
    "\n",
    "运行梯度下降的基本算法不随正则化而改变，它是：\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{repeat until convergence:}\\;\\lbrace\\\\\n",
    "& \\;\\;\\;w_j=w_j-\\alpha\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}\\tag{1}\\;&\\text{for j := 0..n-1}\\\\ \n",
    "& \\;\\;\\;\\;\\;b=b-\\alpha\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}\\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中每次迭代对所有$j$的$w_j$执行同时更新。\n",
    "\n",
    "正则化的变化是计算梯度。"
   ],
   "id": "e8f2a8b5b683e04e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 使用正则化计算梯度（线性/逻辑）\n",
    "\n",
    "线性回归和逻辑回归的梯度计算几乎相同，仅在$f_{\\mathbf{w}b}$的计算上有所不同。\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}&=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})x_{j}^{(i)}+\\frac{\\lambda}{m}w_j\\tag{2}\\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}&=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})\\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m是数据集中训练样本的数量\n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$是模型的预测，而$y^{(i)}$是目标\n",
    "* 对于<span style=\"color:blue\">**线性**</span>回归模型\n",
    "  \n",
    "  $f_{\\mathbf{w},b}(x)=\\mathbf{w}\\cdot\\mathbf{x}+b$\n",
    "* 对于<span style=\"color:blue\">**逻辑**</span>回归模型\n",
    "  \n",
    "  $z=\\mathbf{w}\\cdot\\mathbf{x}+b$\n",
    "\n",
    "  $f_{\\mathbf{w},b}(x)=g(z)$\n",
    "\n",
    "  其中$g(z)$是sigmoid函数：\n",
    "  \n",
    "  $g(z)=\\frac{1}{1+e^{-z}}$\n",
    "\n",
    "添加正则化的项是<span style=\"color:blue\">$\\frac{\\lambda}{m}w_j$</span>。"
   ],
   "id": "f4c3bbdfea2ae83d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 正则化线性回归的梯度函数",
   "id": "a33c1b1865ab91ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.588338Z",
     "start_time": "2024-05-14T15:18:38.574338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient_linear_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "        err = (np.dot(X[i], w) + b) - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err * X[i, j]\n",
    "        dj_db = dj_db + err\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]\n",
    "    return dj_db, dj_dw"
   ],
   "id": "f00457130c2178a2",
   "execution_count": 16,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格以查看其实际情况。",
   "id": "35920b57b7949bfa"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.619369Z",
     "start_time": "2024-05-14T15:18:38.610358Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5, 3)\n",
    "y_tmp = np.array([0, 1, 0, 1, 0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_linear_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ],
   "id": "bc954c83e89dc668",
   "execution_count": 17,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**\n",
    "\n",
    "```\n",
    "dj_db: 0.6648774569425726\n",
    "Regularized dj_dw:\n",
    " [0.29653214748822276, 0.4911679625918033, 0.21645877535865857]\n",
    " ```"
   ],
   "id": "d2593a87df551346"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 正则化逻辑回归的梯度函数",
   "id": "53aa1ccec27b7e1a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.634891Z",
     "start_time": "2024-05-14T15:18:38.621369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient_logistic_reg(X, y, w, b, lambda_):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.0\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
    "        err_i = f_wb_i - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i, j]\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    for j in range(n):\n",
    "        dj_dw[j] = dj_dw[j] + (lambda_ / m) * w[j]\n",
    "    return dj_db, dj_dw"
   ],
   "id": "893d7bf69118d6ce",
   "execution_count": 18,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "运行下面的单元格以查看其实际情况。",
   "id": "b81b82773e7c8136"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.650882Z",
     "start_time": "2024-05-14T15:18:38.636870Z"
    }
   },
   "cell_type": "code",
   "source": [
    "np.random.seed(1)\n",
    "X_tmp = np.random.rand(5, 3)\n",
    "y_tmp = np.array([0, 1, 0, 1, 0])\n",
    "w_tmp = np.random.rand(X_tmp.shape[1])\n",
    "b_tmp = 0.5\n",
    "lambda_tmp = 0.7\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic_reg(X_tmp, y_tmp, w_tmp, b_tmp, lambda_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\", )\n",
    "print(f\"Regularized dj_dw:\\n {dj_dw_tmp.tolist()}\", )"
   ],
   "id": "8a7826c12f34b49f",
   "execution_count": 19,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**\n",
    "\n",
    "```\n",
    "dj_db: 0.341798994972791\n",
    "Regularized dj_dw:\n",
    " [0.17380012933994293, 0.32007507881566943, 0.10776313396851499]\n",
    " ```"
   ],
   "id": "6607ea88f4aa505a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 重新运行过拟合示例",
   "id": "d902d72efa32102c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T15:18:38.852595Z",
     "start_time": "2024-05-14T15:18:38.651881Z"
    }
   },
   "cell_type": "code",
   "source": [
    "plt.close(\"all\")\n",
    "display(output)\n",
    "ofit = overfit_example(True)"
   ],
   "id": "d1c828a949927469",
   "execution_count": 20,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在上图中，尝试对前面的示例进行正则化。尤其：\n",
    "\n",
    "- 分类（逻辑回归）\n",
    "    - 将程度设置为6，将lambda设置为0（无正则化），拟合数据\n",
    "    - 现在将lambda设置为1（增加正则化），拟合数据，注意差异。\n",
    "- 回归（线性回归）\n",
    "    - 尝试相同的过程。"
   ],
   "id": "6a2307d099386534"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 恭喜！\n",
    "\n",
    "你有：\n",
    "\n",
    "- 为线性和逻辑回归添加回归的成本和梯度例程示例\n",
    "- 对正则化如何减少过度拟合产生了一些直觉"
   ],
   "id": "aed56fd3c78a8f29"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
