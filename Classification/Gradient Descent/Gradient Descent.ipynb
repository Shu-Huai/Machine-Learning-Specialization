{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# 可选实验室：逻辑回归的梯度下降",
   "id": "209f2b5f950596f2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 目标\n",
    "\n",
    "在本实验室中，您将：\n",
    "\n",
    "- 更新逻辑回归的梯度下降。\n",
    "- 在熟悉的数据集上探索梯度下降"
   ],
   "id": "38291de662d11454"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:53:39.285076Z",
     "start_time": "2024-05-14T07:53:38.264437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import copy, math\n",
    "import numpy as np\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "from lab_utils_common import dlc, plot_data, plt_tumor_data, sigmoid, compute_cost_logistic\n",
    "from plt_quad_logistic import plt_quad_logistic, plt_prob\n",
    "\n",
    "plt.style.use('./deeplearning.mplstyle')"
   ],
   "id": "d91a8393fdb864d6",
   "execution_count": 1,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 数据集\n",
    "\n",
    "让我们从决策边界实验室中使用的相同的两个特征数据集开始。"
   ],
   "id": "e297e826de287e24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:54:02.623213Z",
     "start_time": "2024-05-14T07:54:02.612200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_train = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ],
   "id": "e4bcba64f87e12db",
   "execution_count": 2,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "和以前一样，我们将使用辅助函数来绘制这些数据。标签$y=1$的数据点显示为红色十字，而标签$y=0$的数据点显示为蓝色圆圈。",
   "id": "435f5bca8c6153cc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T07:54:30.197689Z",
     "start_time": "2024-05-14T07:54:29.971606Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "plot_data(X_train, y_train, ax)\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "ax.set_ylabel('$x_1$', fontsize=12)\n",
    "ax.set_xlabel('$x_0$', fontsize=12)\n",
    "plt.show()"
   ],
   "id": "b348db84a7b881c8",
   "execution_count": 3,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 逻辑梯度下降\n",
    "\n",
    "<img align=\"right\" src=\"../../images/C1_W3_Logistic_gradient_descent.png\" style=\" width:400px; padding: 10px; \" >\n",
    "\n",
    "回想一下梯度下降算法利用梯度计算：\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\text{重复直到收敛：} \\; \\lbrace \\\\\n",
    "&  \\; \\; \\;w_j = w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1} \\\\ \n",
    "&  \\; \\; \\;  \\; \\;b = b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\\\\n",
    "&\\rbrace\n",
    "\\end{align*}$$\n",
    "\n",
    "其中每次迭代对所有$j$对$w_j$执行同时更新，其中\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}&=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})x_{j}^{(i)}\\tag{2}\\\\\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}&=\\frac{1}{m}\\sum\\limits_{i=0}^{m-1}(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})-y^{(i)})\\tag{3} \n",
    "\\end{align*}$$\n",
    "\n",
    "* m是数据集中训练样本的数量\n",
    "* $f_{\\mathbf{w},b}(x^{(i)})$是模型的预测，而$y^{(i)}$是目标\n",
    "* 对于逻辑回归模型\n",
    "  $z=\\mathbf{w}\\cdot\\mathbf{x}+b$\n",
    "  $f_{\\mathbf{w},b}(x)=g(z)$\n",
    "  其中$g(z)$是sigmoid函数：\n",
    "  $g(z)=\\frac{1}{1+e^{-z}}$"
   ],
   "id": "2b1145a464b2d6ee"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 梯度下降实现\n",
    "\n",
    "梯度下降算法的实现有两个组成部分：\n",
    "\n",
    "- 循环执行上面的等式(1)。这就是下面的`gradient_descent`，通常在可选和练习实验室中提供给您。\n",
    "- 当前梯度的计算，上面的方程（2，3）。这是下面的`compute_gradient_logistic`。您将被要求实施本周的实践实验室。\n",
    "\n",
    "#### 计算梯度，代码说明\n",
    "\n",
    "对所有$w_j$和$b$实现上述方程（2）、（3）。\n",
    "\n",
    "有很多方法可以实现这一点。下面概述的是这样的：\n",
    "\n",
    "- 初始化变量以累积`dj_dw`和`dj_db`\n",
    "- 对于每个例子\n",
    "     - 计算该示例的误差$g(\\mathbf{w}\\cdot\\mathbf{x}^{(i)}+b)-\\mathbf{y}^{(i)}$\n",
    "     - 对于本例中的每个输入值$x_{j}^{(i)}$，\n",
    "         - 将误差乘以输入$x_{j}^{(i)}$，并添加到`dj_dw`的相应元素。（上面的等式2）\n",
    "     - 将错误添加到`dj_db`（上面的等式3）\n",
    "- 将`dj_db`和`dj_dw`除以示例总数（m）\n",
    "- 注意NumPy`X[i, :]`或`X[i]`中的$\\mathbf{x}^{(i)}$和$x_{j}^{(i)}$是`X[i, j]`"
   ],
   "id": "5bddd0b170e5100d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:12:56.024025Z",
     "start_time": "2024-05-14T08:12:56.007232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def compute_gradient_logistic(X, y, w, b):\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros((n,))\n",
    "    dj_db = 0.\n",
    "    for i in range(m):\n",
    "        f_wb_i = sigmoid(np.dot(X[i], w) + b)\n",
    "        err_i = f_wb_i - y[i]\n",
    "        for j in range(n):\n",
    "            dj_dw[j] = dj_dw[j] + err_i * X[i, j]\n",
    "        dj_db = dj_db + err_i\n",
    "    dj_dw = dj_dw / m\n",
    "    dj_db = dj_db / m\n",
    "    return dj_db, dj_dw"
   ],
   "id": "b672ab9f42ef4eac",
   "execution_count": 4,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "使用下面的单元格检查梯度函数的实现。",
   "id": "c35682f8df12bf24"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:13:26.112840Z",
     "start_time": "2024-05-14T08:13:26.102833Z"
    }
   },
   "cell_type": "code",
   "source": [
    "X_tmp = np.array([[0.5, 1.5], [1, 1], [1.5, 0.5], [3, 0.5], [2, 2], [1, 2.5]])\n",
    "y_tmp = np.array([0, 0, 0, 1, 1, 1])\n",
    "w_tmp = np.array([2., 3.])\n",
    "b_tmp = 1.\n",
    "dj_db_tmp, dj_dw_tmp = compute_gradient_logistic(X_tmp, y_tmp, w_tmp, b_tmp)\n",
    "print(f\"dj_db: {dj_db_tmp}\")\n",
    "print(f\"dj_dw: {dj_dw_tmp.tolist()}\")"
   ],
   "id": "e1f76f91cdcb95a6",
   "execution_count": 5,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**期望输出**\n",
    "``` \n",
    "dj_db: 0.49861806546328574\n",
    "dj_dw: [0.498333393278696, 0.49883942983996693]\n",
    "```"
   ],
   "id": "c8a7ec6e8900782e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### 梯度下降代码\n",
    "\n",
    "实现上面等式（1）的代码在下面实现。花点时间找到例程中的函数并将其与上面的方程进行比较。"
   ],
   "id": "b7a7f99956008627"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:15:41.991691Z",
     "start_time": "2024-05-14T08:15:41.983188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def gradient_descent(X, y, w_in, b_in, alpha, num_iters):\n",
    "    J_history = []\n",
    "    w = copy.deepcopy(w_in)\n",
    "    b = b_in\n",
    "    for i in range(num_iters):\n",
    "        dj_db, dj_dw = compute_gradient_logistic(X, y, w, b)\n",
    "        w = w - alpha * dj_dw\n",
    "        b = b - alpha * dj_db\n",
    "        J_history.append(compute_cost_logistic(X, y, w, b))\n",
    "        if i % math.ceil(num_iters / 10) == 0:\n",
    "            print(f\"Iteration {i:4d}: Cost {J_history[-1]}   \")\n",
    "    return w, b, J_history"
   ],
   "id": "f3ac6fb0c50d5dae",
   "execution_count": 6,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "让我们在数据集上运行梯度下降。",
   "id": "5345f735dd950ede"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:17:26.139396Z",
     "start_time": "2024-05-14T08:16:34.786747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w_tmp = np.zeros_like(X_train[0])\n",
    "b_tmp = 0.\n",
    "alph = 0.3\n",
    "iters = 1000000\n",
    "w_out, b_out, _ = gradient_descent(X_train, y_train, w_tmp, b_tmp, alph, iters)\n",
    "print(f\"\\nupdated parameters: w:{w_out}, b:{b_out}\")"
   ],
   "id": "8e822e6f5e05db1c",
   "execution_count": 10,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": " #### 让我们绘制梯度下降的结果：",
   "id": "b988dd60980711cf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:18:19.136891Z",
     "start_time": "2024-05-14T08:18:18.952839Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "plt_prob(ax, w_out, b_out)\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.axis([0, 4, 0, 3.5])\n",
    "plot_data(X_train, y_train, ax)\n",
    "x0 = -b_out / w_out[1]\n",
    "x1 = -b_out / w_out[0]\n",
    "ax.plot([0, x0], [x1, 0], c=dlc[\"dlblue\"], lw=1)\n",
    "plt.show()"
   ],
   "id": "f460b44caa9ecb38",
   "execution_count": 11,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在上图中：\n",
    "\n",
    "- 阴影反映概率 y=1（决策边界之前的结果）\n",
    "- 决策边界是概率 = 0.5 的线"
   ],
   "id": "d67f0f2722b7f6c2"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 另一个数据集\n",
    "\n",
    "让我们回到单变量数据集。只需两个参数$w$、$b$，就可以使用等高线图绘制成本函数，以更好地了解梯度下降的作用。"
   ],
   "id": "fb5418de2962c8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:21:59.487334Z",
     "start_time": "2024-05-14T08:21:59.470326Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x_train = np.array([0., 1, 2, 3, 4, 5])\n",
    "y_train = np.array([0, 0, 0, 1, 1, 1])"
   ],
   "id": "d9a9060643c324dd",
   "execution_count": 12,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "和以前一样，我们将使用辅助函数来绘制这些数据。标签$y=1$的数据点显示为红色十字，而标签$y=0$的数据点显示为黑色圆圈。",
   "id": "4a31de99c723944"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:22:27.075653Z",
     "start_time": "2024-05-14T08:22:27.026561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(4, 3))\n",
    "plt_tumor_data(x_train, y_train, ax)\n",
    "plt.show()"
   ],
   "id": "811d154c88df9ee0",
   "execution_count": 13,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "在下图中，尝试：\n",
    "\n",
    "- 通过单击右上角的等值线图来更改$w$和$b$。\n",
    "    - 更改可能需要一两秒钟\n",
    "    - 注意左上角图上成本的变化值。\n",
    "    - 请注意，成本是通过每个示例的损失累积的（垂直虚线）\n",
    "- 单击橙色按钮运行梯度下降。\n",
    "    - 注意成本的稳步下降（等值线和成本图以log(cost)表示）\n",
    "    - 单击等值线图将重置模型以进行新的运行\n",
    "- 要重置绘图，请重新运行单元格"
   ],
   "id": "499d1edcbf09a2f0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-14T08:23:48.696387Z",
     "start_time": "2024-05-14T08:23:48.235493Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w_range = np.array([-1, 7])\n",
    "b_range = np.array([1, -14])\n",
    "quad = plt_quad_logistic(x_train, y_train, w_range, b_range)"
   ],
   "id": "d8ea6be2a4ed6554",
   "execution_count": 14,
   "outputs": []
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 恭喜！\n",
    "\n",
    "你有：\n",
    "\n",
    "- 研究了计算逻辑回归梯度的公式和实现\n",
    "- 利用这些例程\n",
    "    - 探索单个变量数据集\n",
    "    - 探索双变量数据集"
   ],
   "id": "8e312919360156f7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
